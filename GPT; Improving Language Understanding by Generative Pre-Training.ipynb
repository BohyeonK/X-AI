{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf5cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, attn_pdrop):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.dropout = nn.Dropout(attn_pdrop)\n",
    "    \n",
    "    def forward(self, q, k, v, attn_mask):\n",
    "        '''q : (batch_size, n_heads, q_len, d_k)\n",
    "           k : (batch_size, n_heads, k_len, d_k)\n",
    "           v : (batch_size, n_heads, v_len, d_v)\n",
    "           attn_mask : (batch_size, n_heads, q_len, k_len)'''\n",
    "        \n",
    "        attn_score = torch.matmul(q, k.transpose(-1, -2)) / (self.d_k ** 0.5)\n",
    "        attn_score.masked_fill_(attn_mask, -1e9)\n",
    "        '''attn_scroe : (batch_size, n_heads, q_len, k_len)'''\n",
    "        \n",
    "        attn_weights = nn.Softmax(dim=-1)(attn_score)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        '''attn_weights : (batch_size, n_heads, q_len, k_len)'''\n",
    "        \n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        '''output : (batch_size, n_heads, q_len, d_v)'''\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, attn_pdrop):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = self.d_v = d_model//n_heads\n",
    "        \n",
    "        self.WQ = nn.Linear(d_model, d_model)\n",
    "        self.WK = nn.Linear(d_model, d_model)\n",
    "        self.WV = nn.Linear(d_model, d_model)\n",
    "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k, attn_pdrop)\n",
    "        self.linear = nn.Linear(n_heads * self.d_v, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''Q : (batch_size, q_len(=seq_len), d_model)\n",
    "           K : (batch_size, k_len(=seq_len), d_model)\n",
    "           V : (batch_size, v_len(=seq_len), d_model)\n",
    "           attn_mask : (batch_size, q_len, k_len)'''\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        q_heads = self.WQ(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_heads = self.WK(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_heads = self.WV(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "        '''q_heads : (batch_size, n_heads, q_len, d_k)\n",
    "           k_heads : (batch_size, n_heads, k_len, d_k)\n",
    "           v_heads : (batch_size, n_heads, v_len, d_v)'''\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "        '''attn_mask : (batch_size, n_heads, q_len, k_len)'''\n",
    "        attn, attn_weights = self.scaled_dot_product_attn(q_heads, k_heads, v_heads, attn_mask)\n",
    "        '''attn : (batch_size, n_heads, q_len, d_v)\n",
    "           attn_weights : (batch_size, n_heads, q_len, k_len)'''\n",
    "        \n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        '''attn : (batch_size, q_len, n_heads * d_v)'''\n",
    "        outputs = self.linear(attn)\n",
    "        '''outputs : (batch_size, q_len, d_model)'''\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "class PositionWiseFeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        nn.init.normal_(self.linear1.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''inputs : (batch_size, seq_len, d_model)'''\n",
    "\n",
    "        outputs = self.gelu(self.linear1(inputs))\n",
    "        '''outputs : (batch_size, seq_len, d_ff)'''\n",
    "        outputs = self.linear2(outputs)\n",
    "        '''outputs : (batch_size, seq_len, d_model)'''\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, attn_pdrop, resid_pdrop):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, attn_pdrop)\n",
    "        self.dropout1 = nn.Dropout(resid_pdrop)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-5)    \n",
    "\n",
    "        self.ffn = PositionWiseFeedForwardNetwork(d_model, d_ff)\n",
    "        self.dropout2 = nn.Dropout(resid_pdrop)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-5)    \n",
    "\n",
    "    def forward(self, inputs, attn_mask):\n",
    "        '''inputs : (batch_size, seq_len, d_model)\n",
    "           attn_mask : (batch_size, seq_len, seq_len)'''\n",
    "        \n",
    "        attn_outputs, attn_weights = self.mha(inputs, inputs, inputs, attn_mask)\n",
    "        attn_outputs = self.dropout1(attn_outputs)\n",
    "        attn_outputs = self.layernorm1(inputs + attn_outputs)\n",
    "        '''attn_outputs : (batch_size, seq_len, d_model)\n",
    "           attn_weights : (batch_size, n_heads, q_len(=seq_len), k_len(=seq_len))'''\n",
    "        \n",
    "        ffn_outputs = self.ffn(attn_outputs)\n",
    "        ffn_outputs = self.dropout2(ffn_outputs)\n",
    "        ffn_outputs = self.layernorm2(attn_outputs + ffn_outputs)\n",
    "        '''ffn_outputs : (batch_size, seq_len, d_model)'''\n",
    "        \n",
    "        return ffn_outputs, attn_weights\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, d_model, n_layers, n_heads, d_ff, embd_pdrop, attn_pdrop, resid_pdrop, pad_id):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.dropout = nn.Dropout(embd_pdrop)\n",
    "        self.pos_embedding = nn.Embedding(seq_len+1, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, attn_pdrop, resid_pdrop) for _ in range(n_layers)])\n",
    "        \n",
    "        nn.init.normal_(self.embedding.weight, std=0.02)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''inputs : (batch_size, seq_len)'''\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).repeat(inputs.size(0), 1) + 1\n",
    "        position_pad_mask = inputs.eq(self.pad_id)\n",
    "        positions.masked_fill_(position_pad_mask, 0)\n",
    "        '''positions : (batch_size, seq_len)'''\n",
    "\n",
    "        outputs = self.dropout(self.embedding(inputs)) + self.pos_embedding(positions)\n",
    "        '''outputs : (batch_size, seq_len, d_model)'''\n",
    "        \n",
    "        attn_pad_mask = self.get_attention_padding_mask(inputs, inputs, self.pad_id)\n",
    "        '''attn_pad_mask : (batch_size, seq_len, seq_len)'''\n",
    "        subsequent_mask = self.get_attention_subsequent_mask(inputs).to(device=attn_pad_mask.device)\n",
    "        '''subsequent_mask : (batch_size, seq_len, seq_len)'''\n",
    "        attn_mask = torch.gt((attn_pad_mask.to(dtype=subsequent_mask.dtype) + subsequent_mask), 0)\n",
    "        '''attn_mask : (batch_size, seq_len, seq_len)'''\n",
    "        \n",
    "        attention_weights = []\n",
    "        for layer in self.layers:\n",
    "            outputs, attn_weights = layer(outputs, attn_mask)\n",
    "            '''outputs : (batch_size, seq_len, d_model)\n",
    "               attn_weights : (batch_size, n_heads, seq_len, seq_len)'''\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        return outputs, attention_weights    \n",
    "        \n",
    "    def get_attention_padding_mask(self, q, k, pad_id):\n",
    "        attn_pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
    "        '''attn_pad_mask : (batch_size, q_len, k_len)'''\n",
    "\n",
    "        return attn_pad_mask\n",
    "    \n",
    "    def get_attention_subsequent_mask(self, q):\n",
    "        bs, q_len = q.size()\n",
    "        subsequent_mask = torch.ones(bs, q_len, q_len).triu(diagonal=1)\n",
    "        '''subsequent_mask : (batch_size, q_len, q_len)'''\n",
    "        \n",
    "        return subsequent_mask\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 seq_len=512,\n",
    "                 d_model=768,\n",
    "                 n_layers=12,\n",
    "                 n_heads=12,\n",
    "                 d_ff=3072,\n",
    "                 embd_pdrop=0.1,\n",
    "                 attn_pdrop=0.1,\n",
    "                 resid_pdrop=0.1,\n",
    "                 pad_id=0):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.decoder = TransformerDecoder(vocab_size, seq_len, d_model, n_layers, n_heads, d_ff,\n",
    "                                          embd_pdrop, attn_pdrop, resid_pdrop, pad_id)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''inputs : (batch_size, seq_len)'''\n",
    "        \n",
    "        outputs, attention_weights = self.decoder(inputs)\n",
    "        '''outputs : (batch_size, seq_len, d_model)\n",
    "           attention_weights : [(batch_size, n_heads, seq_len, seq_len)] * n_layers'''\n",
    "        \n",
    "        return outputs, attention_weights\n",
    "\n",
    "class GPTLMHead(nn.Module):\n",
    "    def __init__(self, gpt):\n",
    "        super(GPTLMHead, self).__init__()\n",
    "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
    "        \n",
    "        self.gpt = gpt\n",
    "        self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.linear.weight = gpt.decoder.embedding.weight\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''inputs : (batch_size, seq_len)'''\n",
    "\n",
    "        outputs, attention_weights = self.gpt(inputs)\n",
    "        '''outputs : (batch_size, seq_len, d_model)\n",
    "           attention_weights : [(batch_size, n_heads, seq_len, seq_len)] * n_layers'''\n",
    "        \n",
    "        lm_logits = self.linear(outputs)\n",
    "        '''lm_logits : (batch_size, seq_len, vocab_size)'''\n",
    "        \n",
    "        return lm_logits\n",
    "\n",
    "class GPTClsHead(nn.Module):\n",
    "    def __init__(self, gpt, n_class, cls_token_id, cls_pdrop=0.1):\n",
    "        super(GPTClsHead, self).__init__()\n",
    "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
    "        self.cls_token_id = cls_token_id\n",
    "        \n",
    "        self.gpt = gpt\n",
    "        # LM\n",
    "        self.linear1 = nn.Linear(d_model, vocab_size, bias=False) \n",
    "        self.linear1.weight = gpt.decoder.embedding.weight\n",
    "        # Classification\n",
    "        self.linear2 = nn.Linear(d_model, n_class) \n",
    "        self.dropout = nn.Dropout(cls_pdrop)\n",
    "\n",
    "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear2.bias, 0)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''inputs : (batch_size, seq_len)'''\n",
    "\n",
    "        outputs, attention_weights = self.gpt(inputs)\n",
    "        '''outputs : (batch_size, seq_len, d_model)\n",
    "           attention_weights : [(batch_size, n_heads, seq_len, seq_len)] * n_layers'''\n",
    "\n",
    "        lm_logits = self.linear1(outputs)\n",
    "        '''lm_logits : (batch_size, seq_len, vocab_size)'''\n",
    "\n",
    "        outputs = outputs[inputs.eq(self.cls_token_id)]\n",
    "        '''outputs : (batch_size, d_model)'''\n",
    "        cls_logits = self.linear2(self.dropout(outputs))\n",
    "        '''cls_logits : (batch_size, n_class)'''\n",
    "        \n",
    "        return lm_logits, cls_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
