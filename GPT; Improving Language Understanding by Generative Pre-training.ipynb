{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eaf5cf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flowws'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e10b42496450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mflowws\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflowws\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArgument\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mArg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flowws'"
     ]
    }
   ],
   "source": [
    "from typing import Type\n",
    "\n",
    "import flowws\n",
    "from flowws import Argument as Arg\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Layer, Softmax, Embedding, Add, Lambda\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "\n",
    "def universal_transformer_gpt_model(\n",
    "        max_seq_length: int, vocabulary_size: int,\n",
    "        word_embedding_size: int, transformer_depth: int,\n",
    "        num_heads: int, transformer_dropout: float = 0.1,\n",
    "        embedding_dropout: float = 0.6,\n",
    "        l2_reg_penalty: float = 1e-6,\n",
    "        confidence_penalty_weight: float = 0.1,\n",
    "        agglomerative_attention: bool = False,\n",
    "        use_convolutions: bool = False,\n",
    "        use_coordinate_embeddings: bool = True,\n",
    "        convolution_width: int = 0,\n",
    "        penalize_confidence: bool = False,\n",
    "        ):\n",
    "    word_ids = Input(shape=(max_seq_length,), dtype='int32', name='word_ids')\n",
    "    l2_regularizer = (keras.regularizers.l2(l2_reg_penalty) if l2_reg_penalty\n",
    "                      else None)\n",
    "    embedding_layer = ReusableEmbedding(\n",
    "        vocabulary_size, word_embedding_size,\n",
    "        input_length=max_seq_length,\n",
    "        name='bpe_embeddings',\n",
    "        embeddings_regularizer=l2_regularizer)\n",
    "    output_layer = TiedOutputEmbedding(\n",
    "        projection_regularizer=l2_regularizer,\n",
    "        projection_dropout=embedding_dropout,\n",
    "        name='word_prediction_logits')\n",
    "    conv_layer = keras.layers.Conv1D(\n",
    "        word_embedding_size, convolution_width, padding='causal',\n",
    "        activation='relu', kernel_initializer='he_uniform', name='convolution')\n",
    "    coordinate_embedding_layer = TransformerCoordinateEmbedding(\n",
    "        transformer_depth,\n",
    "        name='coordinate_embedding')\n",
    "    transformer_block = TransformerBlock(\n",
    "        name='transformer', num_heads=num_heads,\n",
    "        residual_dropout=transformer_dropout,\n",
    "        attention_dropout=transformer_dropout,\n",
    "        use_masking=True, vanilla_wiring=False,\n",
    "        agglomerative_attention=agglomerative_attention,\n",
    "        )\n",
    "    transformer_act_layer = TransformerACT(name='adaptive_computation_time')\n",
    "    output_softmax_layer = Softmax(name='word_predictions')\n",
    "\n",
    "    next_step_input, embedding_matrix = embedding_layer(word_ids)\n",
    "    act_output = next_step_input\n",
    "\n",
    "    for i in range(transformer_depth):\n",
    "        if use_convolutions:\n",
    "            next_step_input = conv_layer(next_step_input)\n",
    "        if use_coordinate_embeddings:\n",
    "            next_step_input = coordinate_embedding_layer(next_step_input, step=i)\n",
    "        next_step_input = transformer_block(next_step_input)\n",
    "        next_step_input, act_output = transformer_act_layer(next_step_input)\n",
    "\n",
    "    transformer_act_layer.finalize()\n",
    "    next_step_input = act_output\n",
    "    word_predictions = output_softmax_layer(\n",
    "        output_layer([next_step_input, embedding_matrix]))\n",
    "    model = keras.models.Model(inputs=[word_ids], outputs=[word_predictions])\n",
    "    confidence_penalty = K.mean(\n",
    "        confidence_penalty_weight *\n",
    "        K.sum(word_predictions * K.log(word_predictions), axis=-1))\n",
    "    if penalize_confidence:\n",
    "        model.add_loss(confidence_penalty)\n",
    "    return model\n",
    "\n",
    "def vanilla_transformer_gpt_model(\n",
    "        max_seq_length: int, vocabulary_size: int,\n",
    "        word_embedding_size: int, transformer_depth: int,\n",
    "        num_heads: int, transformer_dropout: float = 0.1,\n",
    "        embedding_dropout: float = 0.6,\n",
    "        l2_reg_penalty: float = 1e-6,\n",
    "        confidence_penalty_weight: float = 0.1,\n",
    "        agglomerative_attention: bool = False,\n",
    "        use_convolutions: bool = False,\n",
    "        use_coordinate_embeddings: bool = True,\n",
    "        convolution_width: int = 0,\n",
    "        dropout_cls: Type[Layer] = Dropout\n",
    "        ):\n",
    "    word_ids = Input(shape=(max_seq_length,), dtype='int32', name='word_ids')\n",
    "    l2_regularizer = (keras.regularizers.l2(l2_reg_penalty) if l2_reg_penalty\n",
    "                      else None)\n",
    "    embedding_layer = ReusableEmbedding(\n",
    "        vocabulary_size, word_embedding_size,\n",
    "        input_length=max_seq_length,\n",
    "        name='bpe_embeddings',\n",
    "        embeddings_regularizer=l2_regularizer)\n",
    "    output_layer = TiedOutputEmbedding(\n",
    "        projection_regularizer=l2_regularizer,\n",
    "        projection_dropout=embedding_dropout,\n",
    "        name='word_prediction_logits')\n",
    "    conv_layer = keras.layers.Conv1D(\n",
    "        word_embedding_size, convolution_width, padding='causal',\n",
    "        activation='relu', kernel_initializer='he_uniform', name='convolution')\n",
    "    coordinate_embedding_layer = TransformerCoordinateEmbedding(\n",
    "        1,\n",
    "        name='coordinate_embedding')\n",
    "    output_softmax_layer = Softmax(name='word_predictions')\n",
    "\n",
    "    next_step_input, embedding_matrix = embedding_layer(word_ids)\n",
    "\n",
    "    if use_convolutions:\n",
    "        next_step_input = conv_layer(next_step_input)\n",
    "    if use_coordinate_embeddings:\n",
    "        next_step_input = coordinate_embedding_layer(next_step_input, step=0)\n",
    "    for i in range(transformer_depth):\n",
    "        next_step_input = (\n",
    "            TransformerBlock(\n",
    "                name='transformer' + str(i), num_heads=num_heads,\n",
    "                residual_dropout=transformer_dropout,\n",
    "                attention_dropout=transformer_dropout,\n",
    "                use_masking=True,\n",
    "                vanilla_wiring=True,\n",
    "                agglomerative_attention=agglomerative_attention,\n",
    "                dropout_cls=dropout_cls,\n",
    "            )\n",
    "            (next_step_input))\n",
    "\n",
    "    word_predictions = output_softmax_layer(\n",
    "        output_layer([next_step_input, embedding_matrix]))\n",
    "    model = keras.models.Model(inputs=[word_ids], outputs=[word_predictions])\n",
    "    confidence_penalty = K.mean(\n",
    "        confidence_penalty_weight *\n",
    "        K.sum(word_predictions * K.log(word_predictions), axis=-1))\n",
    "    model.add_loss(confidence_penalty)\n",
    "    return model\n",
    "\n",
    "@flowws.add_stage_arguments\n",
    "class GPTModel(flowws.Stage):\n",
    "    \"\"\"Train a model on the wikitext-2 dataset\"\"\"\n",
    "\n",
    "    ARGS = [\n",
    "        Arg('width', '-w', int, 64,\n",
    "            help='Working width of the deep network'),\n",
    "        Arg('depth', '-d', int, 6,\n",
    "            help='Number of transformer blocks to use'),\n",
    "        Arg('use_convolutions', '-c', bool, False,\n",
    "            help='Use causal convolutions instead of position embeddings'),\n",
    "        Arg('use_agglomeration', '-a', bool, False,\n",
    "            help='Use agglomerative instead of full attention'),\n",
    "        Arg('use_adaptive_computation', None, bool, False,\n",
    "            help='Use adaptive computation time'),\n",
    "        Arg('convolution_width', None, int, 8,\n",
    "            help='Width of causal convolutions to use'),\n",
    "        Arg('num_heads', '-n', int, 8,\n",
    "            help='Number of attention/agglomerative heads to use'),\n",
    "        Arg('print_summary', '-p', bool, False,\n",
    "            help='Print a summary of the model before continuing'),\n",
    "        Arg('dropout', None, float, .5,\n",
    "            help='Dropout to use in transformer layers'),\n",
    "    ]\n",
    "\n",
    "    def run(self, scope, storage):\n",
    "        vocabulary_size = scope['vocabulary_size']\n",
    "        sequence_length = scope['sequence_length']\n",
    "\n",
    "        kwargs = {}\n",
    "\n",
    "        if self.arguments['use_adaptive_computation']:\n",
    "            model = universal_transformer_gpt_model(\n",
    "                sequence_length,\n",
    "                vocabulary_size,\n",
    "                self.arguments['width'],\n",
    "                self.arguments['depth'],\n",
    "                self.arguments['num_heads'],\n",
    "                agglomerative_attention=self.arguments['use_agglomeration'],\n",
    "                use_convolutions=self.arguments['use_convolutions'],\n",
    "                use_coordinate_embeddings=(not self.arguments['use_convolutions']),\n",
    "                convolution_width=self.arguments['convolution_width'],\n",
    "                transformer_dropout=self.arguments['dropout'],\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            if 'dropout_sequence_class' in scope:\n",
    "                kwargs['dropout_cls'] = scope['dropout_sequence_class']\n",
    "\n",
    "            model = vanilla_transformer_gpt_model(\n",
    "                sequence_length,\n",
    "                vocabulary_size,\n",
    "                self.arguments['width'],\n",
    "                self.arguments['depth'],\n",
    "                self.arguments['num_heads'],\n",
    "                agglomerative_attention=self.arguments['use_agglomeration'],\n",
    "                use_convolutions=self.arguments['use_convolutions'],\n",
    "                use_coordinate_embeddings=(not self.arguments['use_convolutions']),\n",
    "                convolution_width=self.arguments['convolution_width'],\n",
    "                transformer_dropout=self.arguments['dropout'],\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        if self.arguments['print_summary']:\n",
    "            model.summary()\n",
    "\n",
    "        scope['model'] = model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
