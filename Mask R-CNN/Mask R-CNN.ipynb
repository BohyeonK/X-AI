{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b53fac",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f0afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mask R-CNN\n",
    "Base Configurations class.\n",
    "Copyright (c) 2017 Matterport, Inc.\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "Written by Waleed Abdulla\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Base Configuration Class\n",
    "# Don't use this class directly. Instead, sub-class it and override\n",
    "# the configurations you need to change.\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Base configuration class. For custom configurations, create a\n",
    "    sub-class that inherits from this one and override properties\n",
    "    that need to be changed.\n",
    "    \"\"\"\n",
    "    # Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.\n",
    "    # Useful if your code needs to do things differently depending on which\n",
    "    # experiment is running.\n",
    "    NAME = None  # Override in sub-classes\n",
    "\n",
    "    # Path to pretrained imagenet model\n",
    "    IMAGENET_MODEL_PATH = os.path.join(os.getcwd(), \"resnet50_imagenet.pth\")\n",
    "\n",
    "    # NUMBER OF GPUs to use. For CPU use 0\n",
    "    GPU_COUNT = 1\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "    # Number of validation steps to run at the end of every training epoch.\n",
    "    # A bigger number improves accuracy of validation stats, but slows\n",
    "    # down the training.\n",
    "    VALIDATION_STEPS = 50\n",
    "\n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a Resnet101 backbone.\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1  # Override in sub-classes\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "    # Anchor stride\n",
    "    # If 1 then anchors are created for each cell in the backbone feature map.\n",
    "    # If 2, then anchors are created for every other cell, and so on.\n",
    "    RPN_ANCHOR_STRIDE = 1\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can reduce this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 256\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 1000\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Input image resing\n",
    "    # Images are resized such that the smallest side is >= IMAGE_MIN_DIM and\n",
    "    # the longest side is <= IMAGE_MAX_DIM. In case both conditions can't\n",
    "    # be satisfied together the IMAGE_MAX_DIM is enforced.\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    # If True, pad images with zeros such that they're (max_dim by max_dim)\n",
    "    IMAGE_PADDING = True  # currently, the False option is not supported\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([123.7, 116.8, 103.9])\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Percent of positive ROIs used to train classifier/mask heads\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "\n",
    "    # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "    MASK_SHAPE = [28, 28]\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 100\n",
    "\n",
    "    # Bounding box refinement standard deviation for RPN and final detections.\n",
    "    RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "\n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "\n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n",
    "    # weights to explode. Likely due to differences in optimzer\n",
    "    # implementation.\n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "\n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "    # Use RPN ROIs or externally generated ROIs for training\n",
    "    # Keep this True for most situations. Set to False if you want to train\n",
    "    # the head branches on ROI generated by code rather than the ROIs from\n",
    "    # the RPN. For example, to debug the classifier head without having to\n",
    "    # train the RPN.\n",
    "    USE_RPN_ROIS = True\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Set values of computed attributes.\"\"\"\n",
    "        # Effective batch size\n",
    "        if self.GPU_COUNT > 0:\n",
    "            self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT\n",
    "        else:\n",
    "            self.BATCH_SIZE = self.IMAGES_PER_GPU\n",
    "\n",
    "        # Adjust step size based on batch size\n",
    "        self.STEPS_PER_EPOCH = self.BATCH_SIZE * self.STEPS_PER_EPOCH\n",
    "\n",
    "        # Input image size\n",
    "        self.IMAGE_SHAPE = np.array(\n",
    "            [self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM, 3])\n",
    "\n",
    "        # Compute backbone size from input image size\n",
    "        self.BACKBONE_SHAPES = np.array(\n",
    "            [[int(math.ceil(self.IMAGE_SHAPE[0] / stride)),\n",
    "              int(math.ceil(self.IMAGE_SHAPE[1] / stride))]\n",
    "             for stride in self.BACKBONE_STRIDES])\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8d3c0",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87032a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mask R-CNN\n",
    "Common utility functions and classes.\n",
    "Copyright (c) 2017 Matterport, Inc.\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "Written by Waleed Abdulla\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import skimage.color\n",
    "import skimage.io\n",
    "import torch\n",
    "\n",
    "############################################################\n",
    "#  Bounding Boxes\n",
    "############################################################\n",
    "\n",
    "def extract_bboxes(mask):\n",
    "    \"\"\"Compute bounding boxes from masks.\n",
    "    mask: [height, width, num_instances]. Mask pixels are either 1 or 0.\n",
    "    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "    \"\"\"\n",
    "    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n",
    "    for i in range(mask.shape[-1]):\n",
    "        m = mask[:, :, i]\n",
    "        # Bounding box.\n",
    "        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n",
    "        vertical_indicies = np.where(np.any(m, axis=1))[0]\n",
    "        if horizontal_indicies.shape[0]:\n",
    "            x1, x2 = horizontal_indicies[[0, -1]]\n",
    "            y1, y2 = vertical_indicies[[0, -1]]\n",
    "            # x2 and y2 should not be part of the box. Increment by 1.\n",
    "            x2 += 1\n",
    "            y2 += 1\n",
    "        else:\n",
    "            # No mask for this instance. Might happen due to\n",
    "            # resizing or cropping. Set bbox to zeros\n",
    "            x1, x2, y1, y2 = 0, 0, 0, 0\n",
    "        boxes[i] = np.array([y1, x1, y2, x2])\n",
    "    return boxes.astype(np.int32)\n",
    "\n",
    "\n",
    "def compute_iou(box, boxes, box_area, boxes_area):\n",
    "    \"\"\"Calculates IoU of the given box with the array of the given boxes.\n",
    "    box: 1D vector [y1, x1, y2, x2]\n",
    "    boxes: [boxes_count, (y1, x1, y2, x2)]\n",
    "    box_area: float. the area of 'box'\n",
    "    boxes_area: array of length boxes_count.\n",
    "    Note: the areas are passed in rather than calculated here for\n",
    "          efficency. Calculate once in the caller to avoid duplicate work.\n",
    "    \"\"\"\n",
    "    # Calculate intersection areas\n",
    "    y1 = np.maximum(box[0], boxes[:, 0])\n",
    "    y2 = np.minimum(box[2], boxes[:, 2])\n",
    "    x1 = np.maximum(box[1], boxes[:, 1])\n",
    "    x2 = np.minimum(box[3], boxes[:, 3])\n",
    "    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "    union = box_area + boxes_area[:] - intersection[:]\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "\n",
    "def compute_overlaps(boxes1, boxes2):\n",
    "    \"\"\"Computes IoU overlaps between two sets of boxes.\n",
    "    boxes1, boxes2: [N, (y1, x1, y2, x2)].\n",
    "    For better performance, pass the largest set first and the smaller second.\n",
    "    \"\"\"\n",
    "    # Areas of anchors and GT boxes\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n",
    "    # Each cell contains the IoU value.\n",
    "    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n",
    "    for i in range(overlaps.shape[1]):\n",
    "        box2 = boxes2[i]\n",
    "        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n",
    "    return overlaps\n",
    "\n",
    "def box_refinement(box, gt_box):\n",
    "    \"\"\"Compute refinement needed to transform box to gt_box.\n",
    "    box and gt_box are [N, (y1, x1, y2, x2)]\n",
    "    \"\"\"\n",
    "\n",
    "    height = box[:, 2] - box[:, 0]\n",
    "    width = box[:, 3] - box[:, 1]\n",
    "    center_y = box[:, 0] + 0.5 * height\n",
    "    center_x = box[:, 1] + 0.5 * width\n",
    "\n",
    "    gt_height = gt_box[:, 2] - gt_box[:, 0]\n",
    "    gt_width = gt_box[:, 3] - gt_box[:, 1]\n",
    "    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n",
    "    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n",
    "\n",
    "    dy = (gt_center_y - center_y) / height\n",
    "    dx = (gt_center_x - center_x) / width\n",
    "    dh = torch.log(gt_height / height)\n",
    "    dw = torch.log(gt_width / width)\n",
    "\n",
    "    result = torch.stack([dy, dx, dh, dw], dim=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Dataset\n",
    "############################################################\n",
    "\n",
    "class Dataset(object):\n",
    "    \"\"\"The base class for dataset classes.\n",
    "    To use it, create a new class that adds functions specific to the dataset\n",
    "    you want to use. For example:\n",
    "    class CatsAndDogsDataset(Dataset):\n",
    "        def load_cats_and_dogs(self):\n",
    "            ...\n",
    "        def load_mask(self, image_id):\n",
    "            ...\n",
    "        def image_reference(self, image_id):\n",
    "            ...\n",
    "    See COCODataset and ShapesDataset as examples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_map=None):\n",
    "        self._image_ids = []\n",
    "        self.image_info = []\n",
    "        # Background is always the first class\n",
    "        self.class_info = [{\"source\": \"\", \"id\": 0, \"name\": \"BG\"}]\n",
    "        self.source_class_ids = {}\n",
    "\n",
    "    def add_class(self, source, class_id, class_name):\n",
    "        assert \".\" not in source, \"Source name cannot contain a dot\"\n",
    "        # Does the class exist already?\n",
    "        for info in self.class_info:\n",
    "            if info['source'] == source and info[\"id\"] == class_id:\n",
    "                # source.class_id combination already available, skip\n",
    "                return\n",
    "        # Add the class\n",
    "        self.class_info.append({\n",
    "            \"source\": source,\n",
    "            \"id\": class_id,\n",
    "            \"name\": class_name,\n",
    "        })\n",
    "\n",
    "    def add_image(self, source, image_id, path, **kwargs):\n",
    "        image_info = {\n",
    "            \"id\": image_id,\n",
    "            \"source\": source,\n",
    "            \"path\": path,\n",
    "        }\n",
    "        image_info.update(kwargs)\n",
    "        self.image_info.append(image_info)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return a link to the image in its source Website or details about\n",
    "        the image that help looking it up or debugging it.\n",
    "        Override for your dataset, but pass to this function\n",
    "        if you encounter images not in your dataset.\n",
    "        \"\"\"\n",
    "        return \"\"\n",
    "\n",
    "    def prepare(self, class_map=None):\n",
    "        \"\"\"Prepares the Dataset class for use.\n",
    "        TODO: class map is not supported yet. When done, it should handle mapping\n",
    "              classes from different datasets to the same class ID.\n",
    "        \"\"\"\n",
    "        def clean_name(name):\n",
    "            \"\"\"Returns a shorter version of object names for cleaner display.\"\"\"\n",
    "            return \",\".join(name.split(\",\")[:1])\n",
    "\n",
    "        # Build (or rebuild) everything else from the info dicts.\n",
    "        self.num_classes = len(self.class_info)\n",
    "        self.class_ids = np.arange(self.num_classes)\n",
    "        self.class_names = [clean_name(c[\"name\"]) for c in self.class_info]\n",
    "        self.num_images = len(self.image_info)\n",
    "        self._image_ids = np.arange(self.num_images)\n",
    "\n",
    "        self.class_from_source_map = {\"{}.{}\".format(info['source'], info['id']): id\n",
    "                                      for info, id in zip(self.class_info, self.class_ids)}\n",
    "\n",
    "        # Map sources to class_ids they support\n",
    "        self.sources = list(set([i['source'] for i in self.class_info]))\n",
    "        self.source_class_ids = {}\n",
    "        # Loop over datasets\n",
    "        for source in self.sources:\n",
    "            self.source_class_ids[source] = []\n",
    "            # Find classes that belong to this dataset\n",
    "            for i, info in enumerate(self.class_info):\n",
    "                # Include BG class in all datasets\n",
    "                if i == 0 or source == info['source']:\n",
    "                    self.source_class_ids[source].append(i)\n",
    "\n",
    "    def map_source_class_id(self, source_class_id):\n",
    "        \"\"\"Takes a source class ID and returns the int class ID assigned to it.\n",
    "        For example:\n",
    "        dataset.map_source_class_id(\"coco.12\") -> 23\n",
    "        \"\"\"\n",
    "        return self.class_from_source_map[source_class_id]\n",
    "\n",
    "    def get_source_class_id(self, class_id, source):\n",
    "        \"\"\"Map an internal class ID to the corresponding class ID in the source dataset.\"\"\"\n",
    "        info = self.class_info[class_id]\n",
    "        assert info['source'] == source\n",
    "        return info['id']\n",
    "\n",
    "    def append_data(self, class_info, image_info):\n",
    "        self.external_to_class_id = {}\n",
    "        for i, c in enumerate(self.class_info):\n",
    "            for ds, id in c[\"map\"]:\n",
    "                self.external_to_class_id[ds + str(id)] = i\n",
    "\n",
    "        # Map external image IDs to internal ones.\n",
    "        self.external_to_image_id = {}\n",
    "        for i, info in enumerate(self.image_info):\n",
    "            self.external_to_image_id[info[\"ds\"] + str(info[\"id\"])] = i\n",
    "\n",
    "    @property\n",
    "    def image_ids(self):\n",
    "        return self._image_ids\n",
    "\n",
    "    def source_image_link(self, image_id):\n",
    "        \"\"\"Returns the path or URL to the image.\n",
    "        Override this to return a URL to the image if it's availble online for easy\n",
    "        debugging.\n",
    "        \"\"\"\n",
    "        return self.image_info[image_id][\"path\"]\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image = skimage.io.imread(self.image_info[image_id]['path'])\n",
    "        # If grayscale. Convert to RGB for consistency.\n",
    "        if image.ndim != 3:\n",
    "            image = skimage.color.gray2rgb(image)\n",
    "        return image\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. Override this\n",
    "        method to load instance masks and return them in the form of am\n",
    "        array of binary masks of shape [height, width, instances].\n",
    "        Returns:\n",
    "            masks: A bool array of shape [height, width, instance count] with\n",
    "                a binary mask per instance.\n",
    "            class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # Override this function to load a mask from your dataset.\n",
    "        # Otherwise, it returns an empty mask.\n",
    "        mask = np.empty([0, 0, 0])\n",
    "        class_ids = np.empty([0], np.int32)\n",
    "        return mask, class_ids\n",
    "\n",
    "\n",
    "def resize_image(image, min_dim=None, max_dim=None, padding=False):\n",
    "    \"\"\"\n",
    "    Resizes an image keeping the aspect ratio.\n",
    "    min_dim: if provided, resizes the image such that it's smaller\n",
    "        dimension == min_dim\n",
    "    max_dim: if provided, ensures that the image longest side doesn't\n",
    "        exceed this value.\n",
    "    padding: If true, pads image with zeros so it's size is max_dim x max_dim\n",
    "    Returns:\n",
    "    image: the resized image\n",
    "    window: (y1, x1, y2, x2). If max_dim is provided, padding might\n",
    "        be inserted in the returned image. If so, this window is the\n",
    "        coordinates of the image part of the full image (excluding\n",
    "        the padding). The x2, y2 pixels are not included.\n",
    "    scale: The scale factor used to resize the image\n",
    "    padding: Padding added to the image [(top, bottom), (left, right), (0, 0)]\n",
    "    \"\"\"\n",
    "    # Default window (y1, x1, y2, x2) and default scale == 1.\n",
    "    h, w = image.shape[:2]\n",
    "    window = (0, 0, h, w)\n",
    "    scale = 1\n",
    "\n",
    "    # Scale?\n",
    "    if min_dim:\n",
    "        # Scale up but not down\n",
    "        scale = max(1, min_dim / min(h, w))\n",
    "    # Does it exceed max dim?\n",
    "    if max_dim:\n",
    "        image_max = max(h, w)\n",
    "        if round(image_max * scale) > max_dim:\n",
    "            scale = max_dim / image_max\n",
    "    # Resize image and mask\n",
    "    if scale != 1:\n",
    "        image = scipy.misc.imresize(\n",
    "            image, (round(h * scale), round(w * scale)))\n",
    "    # Need padding?\n",
    "    if padding:\n",
    "        # Get new height and width\n",
    "        h, w = image.shape[:2]\n",
    "        top_pad = (max_dim - h) // 2\n",
    "        bottom_pad = max_dim - h - top_pad\n",
    "        left_pad = (max_dim - w) // 2\n",
    "        right_pad = max_dim - w - left_pad\n",
    "        padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]\n",
    "        image = np.pad(image, padding, mode='constant', constant_values=0)\n",
    "        window = (top_pad, left_pad, h + top_pad, w + left_pad)\n",
    "    return image, window, scale, padding\n",
    "\n",
    "\n",
    "def resize_mask(mask, scale, padding):\n",
    "    \"\"\"Resizes a mask using the given scale and padding.\n",
    "    Typically, you get the scale and padding from resize_image() to\n",
    "    ensure both, the image and the mask, are resized consistently.\n",
    "    scale: mask scaling factor\n",
    "    padding: Padding to add to the mask in the form\n",
    "            [(top, bottom), (left, right), (0, 0)]\n",
    "    \"\"\"\n",
    "    h, w = mask.shape[:2]\n",
    "    mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)\n",
    "    mask = np.pad(mask, padding, mode='constant', constant_values=0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def minimize_mask(bbox, mask, mini_shape):\n",
    "    \"\"\"Resize masks to a smaller version to cut memory load.\n",
    "    Mini-masks can then resized back to image scale using expand_masks()\n",
    "    See inspect_data.ipynb notebook for more details.\n",
    "    \"\"\"\n",
    "    mini_mask = np.zeros(mini_shape + (mask.shape[-1],), dtype=bool)\n",
    "    for i in range(mask.shape[-1]):\n",
    "        m = mask[:, :, i]\n",
    "        y1, x1, y2, x2 = bbox[i][:4]\n",
    "        m = m[y1:y2, x1:x2]\n",
    "        if m.size == 0:\n",
    "            raise Exception(\"Invalid bounding box with area of zero\")\n",
    "        m = scipy.misc.imresize(m.astype(float), mini_shape, interp='bilinear')\n",
    "        mini_mask[:, :, i] = np.where(m >= 128, 1, 0)\n",
    "    return mini_mask\n",
    "\n",
    "\n",
    "def expand_mask(bbox, mini_mask, image_shape):\n",
    "    \"\"\"Resizes mini masks back to image size. Reverses the change\n",
    "    of minimize_mask().\n",
    "    See inspect_data.ipynb notebook for more details.\n",
    "    \"\"\"\n",
    "    mask = np.zeros(image_shape[:2] + (mini_mask.shape[-1],), dtype=bool)\n",
    "    for i in range(mask.shape[-1]):\n",
    "        m = mini_mask[:, :, i]\n",
    "        y1, x1, y2, x2 = bbox[i][:4]\n",
    "        h = y2 - y1\n",
    "        w = x2 - x1\n",
    "        m = scipy.misc.imresize(m.astype(float), (h, w), interp='bilinear')\n",
    "        mask[y1:y2, x1:x2, i] = np.where(m >= 128, 1, 0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "# TODO: Build and use this function to reduce code duplication\n",
    "def mold_mask(mask, config):\n",
    "    pass\n",
    "\n",
    "\n",
    "def unmold_mask(mask, bbox, image_shape):\n",
    "    \"\"\"Converts a mask generated by the neural network into a format similar\n",
    "    to it's original shape.\n",
    "    mask: [height, width] of type float. A small, typically 28x28 mask.\n",
    "    bbox: [y1, x1, y2, x2]. The box to fit the mask in.\n",
    "    Returns a binary mask with the same size as the original image.\n",
    "    \"\"\"\n",
    "    threshold = 0.5\n",
    "    y1, x1, y2, x2 = bbox\n",
    "    mask = scipy.misc.imresize(\n",
    "        mask, (y2 - y1, x2 - x1), interp='bilinear').astype(np.float32) / 255.0\n",
    "    mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
    "\n",
    "    # Put the mask in the right location.\n",
    "    full_mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "    full_mask[y1:y2, x1:x2] = mask\n",
    "    return full_mask\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Anchors\n",
    "############################################################\n",
    "\n",
    "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n",
    "    \"\"\"\n",
    "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
    "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
    "    shape: [height, width] spatial shape of the feature map over which\n",
    "            to generate anchors.\n",
    "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
    "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
    "        value is 2 then generate anchors for every other feature map pixel.\n",
    "    \"\"\"\n",
    "    # Get all combinations of scales and ratios\n",
    "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
    "    scales = scales.flatten()\n",
    "    ratios = ratios.flatten()\n",
    "\n",
    "    # Enumerate heights and widths from scales and ratios\n",
    "    heights = scales / np.sqrt(ratios)\n",
    "    widths = scales * np.sqrt(ratios)\n",
    "\n",
    "    # Enumerate shifts in feature space\n",
    "    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
    "\n",
    "    # Enumerate combinations of shifts, widths, and heights\n",
    "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
    "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
    "\n",
    "    # Reshape to get a list of (y, x) and a list of (h, w)\n",
    "    box_centers = np.stack(\n",
    "        [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n",
    "    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n",
    "\n",
    "    # Convert to corner coordinates (y1, x1, y2, x2)\n",
    "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
    "                            box_centers + 0.5 * box_sizes], axis=1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,\n",
    "                             anchor_stride):\n",
    "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
    "    is associated with a level of the pyramid, but each ratio is used in\n",
    "    all levels of the pyramid.\n",
    "    Returns:\n",
    "    anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted\n",
    "        with the same order of the given scales. So, anchors of scale[0] come\n",
    "        first, then anchors of scale[1], and so on.\n",
    "    \"\"\"\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, y2, x2)]\n",
    "    anchors = []\n",
    "    for i in range(len(scales)):\n",
    "        anchors.append(generate_anchors(scales[i], ratios, feature_shapes[i],\n",
    "                                        feature_strides[i], anchor_stride))\n",
    "    return np.concatenate(anchors, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c689c",
   "metadata": {},
   "source": [
    "# coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efb6293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --dataset /path/to/coco/ [--year <year>] [--model /path/to/weights.pth]\n",
      "                             [--logs /path/to/logs/] [--limit <image count>] [--download <True|False>]\n",
      "                             <command>\n",
      "ipykernel_launcher.py: error: the following arguments are required: --dataset\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhfux\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mask R-CNN\n",
    "Configurations and data loading code for MS COCO.\n",
    "Copyright (c) 2017 Matterport, Inc.\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "Written by Waleed Abdulla\n",
    "------------------------------------------------------------\n",
    "Usage: import the module (see Jupyter notebooks for examples), or run from\n",
    "       the command line as such:\n",
    "    # Train a new model starting from pre-trained COCO weights\n",
    "    python3 coco.py train --dataset=/path/to/coco/ --model=coco\n",
    "    # Train a new model starting from ImageNet weights\n",
    "    python3 coco.py train --dataset=/path/to/coco/ --model=imagenet\n",
    "    # Continue training a model that you had trained earlier\n",
    "    python3 coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5\n",
    "    # Continue training the last model you trained\n",
    "    python3 coco.py train --dataset=/path/to/coco/ --model=last\n",
    "    # Run COCO evaluatoin on the last model you trained\n",
    "    python3 coco.py evaluate --dataset=/path/to/coco/ --model=last\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Download and install the Python COCO tools from https://github.com/waleedka/coco\n",
    "# That's a fork from the original https://github.com/pdollar/coco with a bug\n",
    "# fix for Python 3.\n",
    "# I submitted a pull request https://github.com/cocodataset/cocoapi/pull/50\n",
    "# If the PR is merged then use the original repo.\n",
    "# Note: Edit PythonAPI/Makefile and replace \"python\" with \"python3\".\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.pth\")\n",
    "\n",
    "# Directory to save logs and model checkpoints, if not provided\n",
    "# through the command line argument --logs\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "DEFAULT_DATASET_YEAR = \"2014\"\n",
    "\n",
    "############################################################\n",
    "#  Configurations\n",
    "############################################################\n",
    "\n",
    "class CocoConfig(Config):\n",
    "    \"\"\"Configuration for training on MS COCO.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the COCO dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"coco\"\n",
    "\n",
    "    # We use one GPU with 8GB memory, which can fit one image.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 16\n",
    "\n",
    "    # Uncomment to train on 8 GPUs (default is 1)\n",
    "    # GPU_COUNT = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 80  # COCO has 80 classes\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Dataset\n",
    "############################################################\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def load_coco(self, dataset_dir, subset, year=DEFAULT_DATASET_YEAR, class_ids=None,\n",
    "                  class_map=None, return_coco=False, auto_download=False):\n",
    "        \"\"\"Load a subset of the COCO dataset.\n",
    "        dataset_dir: The root directory of the COCO dataset.\n",
    "        subset: What to load (train, val, minival, valminusminival)\n",
    "        year: What dataset year to load (2014, 2017) as a string, not an integer\n",
    "        class_ids: If provided, only loads images that have the given classes.\n",
    "        class_map: TODO: Not implemented yet. Supports maping classes from\n",
    "            different datasets to the same class ID.\n",
    "        return_coco: If True, returns the COCO object.\n",
    "        auto_download: Automatically download and unzip MS-COCO images and annotations\n",
    "        \"\"\"\n",
    "\n",
    "        if auto_download is True:\n",
    "            self.auto_download(dataset_dir, subset, year)\n",
    "\n",
    "        coco = COCO(\"{}/annotations/instances_{}{}.json\".format(dataset_dir, subset, year))\n",
    "        if subset == \"minival\" or subset == \"valminusminival\":\n",
    "            subset = \"val\"\n",
    "        image_dir = \"{}/{}{}\".format(dataset_dir, subset, year)\n",
    "\n",
    "        # Load all classes or a subset?\n",
    "        if not class_ids:\n",
    "            # All classes\n",
    "            class_ids = sorted(coco.getCatIds())\n",
    "\n",
    "        # All images or a subset?\n",
    "        if class_ids:\n",
    "            image_ids = []\n",
    "            for id in class_ids:\n",
    "                image_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
    "            # Remove duplicates\n",
    "            image_ids = list(set(image_ids))\n",
    "        else:\n",
    "            # All images\n",
    "            image_ids = list(coco.imgs.keys())\n",
    "\n",
    "        # Add classes\n",
    "        for i in class_ids:\n",
    "            self.add_class(\"coco\", i, coco.loadCats(i)[0][\"name\"])\n",
    "\n",
    "        # Add images\n",
    "        for i in image_ids:\n",
    "            self.add_image(\n",
    "                \"coco\", image_id=i,\n",
    "                path=os.path.join(image_dir, coco.imgs[i]['file_name']),\n",
    "                width=coco.imgs[i][\"width\"],\n",
    "                height=coco.imgs[i][\"height\"],\n",
    "                annotations=coco.loadAnns(coco.getAnnIds(\n",
    "                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
    "        if return_coco:\n",
    "            return coco\n",
    "\n",
    "    def auto_download(self, dataDir, dataType, dataYear):\n",
    "        \"\"\"Download the COCO dataset/annotations if requested.\n",
    "        dataDir: The root directory of the COCO dataset.\n",
    "        dataType: What to load (train, val, minival, valminusminival)\n",
    "        dataYear: What dataset year to load (2014, 2017) as a string, not an integer\n",
    "        Note:\n",
    "            For 2014, use \"train\", \"val\", \"minival\", or \"valminusminival\"\n",
    "            For 2017, only \"train\" and \"val\" annotations are available\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup paths and file names\n",
    "        if dataType == \"minival\" or dataType == \"valminusminival\":\n",
    "            imgDir = \"{}/{}{}\".format(dataDir, \"val\", dataYear)\n",
    "            imgZipFile = \"{}/{}{}.zip\".format(dataDir, \"val\", dataYear)\n",
    "            imgURL = \"http://images.cocodataset.org/zips/{}{}.zip\".format(\"val\", dataYear)\n",
    "        else:\n",
    "            imgDir = \"{}/{}{}\".format(dataDir, dataType, dataYear)\n",
    "            imgZipFile = \"{}/{}{}.zip\".format(dataDir, dataType, dataYear)\n",
    "            imgURL = \"http://images.cocodataset.org/zips/{}{}.zip\".format(dataType, dataYear)\n",
    "        # print(\"Image paths:\"); print(imgDir); print(imgZipFile); print(imgURL)\n",
    "\n",
    "        # Create main folder if it doesn't exist yet\n",
    "        if not os.path.exists(dataDir):\n",
    "            os.makedirs(dataDir)\n",
    "\n",
    "        # Download images if not available locally\n",
    "        if not os.path.exists(imgDir):\n",
    "            os.makedirs(imgDir)\n",
    "            print(\"Downloading images to \" + imgZipFile + \" ...\")\n",
    "            with urllib.request.urlopen(imgURL) as resp, open(imgZipFile, 'wb') as out:\n",
    "                shutil.copyfileobj(resp, out)\n",
    "            print(\"... done downloading.\")\n",
    "            print(\"Unzipping \" + imgZipFile)\n",
    "            with zipfile.ZipFile(imgZipFile, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(dataDir)\n",
    "            print(\"... done unzipping\")\n",
    "        print(\"Will use images in \" + imgDir)\n",
    "\n",
    "        # Setup annotations data paths\n",
    "        annDir = \"{}/annotations\".format(dataDir)\n",
    "        if dataType == \"minival\":\n",
    "            annZipFile = \"{}/instances_minival2014.json.zip\".format(dataDir)\n",
    "            annFile = \"{}/instances_minival2014.json\".format(annDir)\n",
    "            annURL = \"https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?dl=0\"\n",
    "            unZipDir = annDir\n",
    "        elif dataType == \"valminusminival\":\n",
    "            annZipFile = \"{}/instances_valminusminival2014.json.zip\".format(dataDir)\n",
    "            annFile = \"{}/instances_valminusminival2014.json\".format(annDir)\n",
    "            annURL = \"https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?dl=0\"\n",
    "            unZipDir = annDir\n",
    "        else:\n",
    "            annZipFile = \"{}/annotations_trainval{}.zip\".format(dataDir, dataYear)\n",
    "            annFile = \"{}/instances_{}{}.json\".format(annDir, dataType, dataYear)\n",
    "            annURL = \"http://images.cocodataset.org/annotations/annotations_trainval{}.zip\".format(dataYear)\n",
    "            unZipDir = dataDir\n",
    "        # print(\"Annotations paths:\"); print(annDir); print(annFile); print(annZipFile); print(annURL)\n",
    "\n",
    "        # Download annotations if not available locally\n",
    "        if not os.path.exists(annDir):\n",
    "            os.makedirs(annDir)\n",
    "        if not os.path.exists(annFile):\n",
    "            if not os.path.exists(annZipFile):\n",
    "                print(\"Downloading zipped annotations to \" + annZipFile + \" ...\")\n",
    "                with urllib.request.urlopen(annURL) as resp, open(annZipFile, 'wb') as out:\n",
    "                    shutil.copyfileobj(resp, out)\n",
    "                print(\"... done downloading.\")\n",
    "            print(\"Unzipping \" + annZipFile)\n",
    "            with zipfile.ZipFile(annZipFile, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(unZipDir)\n",
    "            print(\"... done unzipping\")\n",
    "        print(\"Will use annotations in \" + annFile)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"coco\":\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"coco.{}\".format(annotation['category_id']))\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"coco\":\n",
    "            return \"http://cocodataset.org/#explore?id={}\".format(info[\"id\"])\n",
    "        else:\n",
    "            super(CocoDataset, self).image_reference(image_id)\n",
    "\n",
    "    # The following two functions are from pycocotools with a few changes.\n",
    "\n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  COCO Evaluation\n",
    "############################################################\n",
    "\n",
    "def build_coco_results(dataset, image_ids, rois, class_ids, scores, masks):\n",
    "    \"\"\"Arrange resutls to match COCO specs in http://cocodataset.org/#format\n",
    "    \"\"\"\n",
    "    # If no results, return an empty list\n",
    "    if rois is None:\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for image_id in image_ids:\n",
    "        # Loop through detections\n",
    "        for i in range(rois.shape[0]):\n",
    "            class_id = class_ids[i]\n",
    "            score = scores[i]\n",
    "            bbox = np.around(rois[i], 1)\n",
    "            mask = masks[:, :, i]\n",
    "\n",
    "            result = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": dataset.get_source_class_id(class_id, \"coco\"),\n",
    "                \"bbox\": [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]],\n",
    "                \"score\": score,\n",
    "                \"segmentation\": maskUtils.encode(np.asfortranarray(mask))\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_coco(model, dataset, coco, eval_type=\"bbox\", limit=0, image_ids=None):\n",
    "    \"\"\"Runs official COCO evaluation.\n",
    "    dataset: A Dataset object with valiadtion data\n",
    "    eval_type: \"bbox\" or \"segm\" for bounding box or segmentation evaluation\n",
    "    limit: if not 0, it's the number of images to use for evaluation\n",
    "    \"\"\"\n",
    "    # Pick COCO images from the dataset\n",
    "    image_ids = image_ids or dataset.image_ids\n",
    "\n",
    "    # Limit to a subset\n",
    "    if limit:\n",
    "        image_ids = image_ids[:limit]\n",
    "\n",
    "    # Get corresponding COCO image IDs.\n",
    "    coco_image_ids = [dataset.image_info[id][\"id\"] for id in image_ids]\n",
    "\n",
    "    t_prediction = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    results = []\n",
    "    for i, image_id in enumerate(image_ids):\n",
    "        # Load image\n",
    "        image = dataset.load_image(image_id)\n",
    "\n",
    "        # Run detection\n",
    "        t = time.time()\n",
    "        r = model.detect([image])[0]\n",
    "        t_prediction += (time.time() - t)\n",
    "\n",
    "        # Convert results to COCO format\n",
    "        image_results = build_coco_results(dataset, coco_image_ids[i:i + 1],\n",
    "                                           r[\"rois\"], r[\"class_ids\"],\n",
    "                                           r[\"scores\"], r[\"masks\"])\n",
    "        results.extend(image_results)\n",
    "\n",
    "    # Load results. This modifies results with additional attributes.\n",
    "    coco_results = coco.loadRes(results)\n",
    "\n",
    "    # Evaluate\n",
    "    cocoEval = COCOeval(coco, coco_results, eval_type)\n",
    "    cocoEval.params.imgIds = coco_image_ids\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "\n",
    "    print(\"Prediction time: {}. Average {}/image\".format(\n",
    "        t_prediction, t_prediction / len(image_ids)))\n",
    "    print(\"Total time: \", time.time() - t_start)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Training\n",
    "############################################################\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Train Mask R-CNN on MS COCO.')\n",
    "    parser.add_argument(\"command\",\n",
    "                        metavar=\"<command>\",\n",
    "                        help=\"'train' or 'evaluate' on MS COCO\")\n",
    "    parser.add_argument('--dataset', required=True,\n",
    "                        metavar=\"/path/to/coco/\",\n",
    "                        help='Directory of the MS-COCO dataset')\n",
    "    parser.add_argument('--year', required=False,\n",
    "                        default=DEFAULT_DATASET_YEAR,\n",
    "                        metavar=\"<year>\",\n",
    "                        help='Year of the MS-COCO dataset (2014 or 2017) (default=2014)')\n",
    "    parser.add_argument('--model', required=False,\n",
    "                        metavar=\"/path/to/weights.pth\",\n",
    "                        help=\"Path to weights .pth file or 'coco'\")\n",
    "    parser.add_argument('--logs', required=False,\n",
    "                        default=DEFAULT_LOGS_DIR,\n",
    "                        metavar=\"/path/to/logs/\",\n",
    "                        help='Logs and checkpoints directory (default=logs/)')\n",
    "    parser.add_argument('--limit', required=False,\n",
    "                        default=500,\n",
    "                        metavar=\"<image count>\",\n",
    "                        help='Images to use for evaluation (default=500)')\n",
    "    parser.add_argument('--download', required=False,\n",
    "                        default=False,\n",
    "                        metavar=\"<True|False>\",\n",
    "                        help='Automatically download and unzip MS-COCO files (default=False)',\n",
    "                        type=bool)\n",
    "    args = parser.parse_args()\n",
    "    print(\"Command: \", args.command)\n",
    "    print(\"Model: \", args.model)\n",
    "    print(\"Dataset: \", args.dataset)\n",
    "    print(\"Year: \", args.year)\n",
    "    print(\"Logs: \", args.logs)\n",
    "    print(\"Auto Download: \", args.download)\n",
    "\n",
    "    # Configurations\n",
    "    if args.command == \"train\":\n",
    "        config = CocoConfig()\n",
    "    else:\n",
    "        class InferenceConfig(CocoConfig):\n",
    "            # Set batch size to 1 since we'll be running inference on\n",
    "            # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "            GPU_COUNT = 1\n",
    "            IMAGES_PER_GPU = 1\n",
    "            DETECTION_MIN_CONFIDENCE = 0\n",
    "        config = InferenceConfig()\n",
    "    config.display()\n",
    "\n",
    "    # Create model\n",
    "    if args.command == \"train\":\n",
    "        model = MaskRCNN(config=config,\n",
    "                                  model_dir=args.logs)\n",
    "    else:\n",
    "        model = MaskRCNN(config=config,\n",
    "                                  model_dir=args.logs)\n",
    "    if config.GPU_COUNT:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Select weights file to load\n",
    "    if args.model:\n",
    "        if args.model.lower() == \"coco\":\n",
    "            model_path = COCO_MODEL_PATH\n",
    "        elif args.model.lower() == \"last\":\n",
    "            # Find last trained weights\n",
    "            model_path = model.find_last()[1]\n",
    "        elif args.model.lower() == \"imagenet\":\n",
    "            # Start from ImageNet trained weights\n",
    "            model_path = config.IMAGENET_MODEL_PATH\n",
    "        else:\n",
    "            model_path = args.model\n",
    "    else:\n",
    "        model_path = \"\"\n",
    "\n",
    "    # Load weights\n",
    "    print(\"Loading weights \", model_path)\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    # Train or evaluate\n",
    "    if args.command == \"train\":\n",
    "        # Training dataset. Use the training set and 35K from the\n",
    "        # validation set, as as in the Mask RCNN paper.\n",
    "        dataset_train = CocoDataset()\n",
    "        dataset_train.load_coco(args.dataset, \"train\", year=args.year, auto_download=args.download)\n",
    "        dataset_train.load_coco(args.dataset, \"valminusminival\", year=args.year, auto_download=args.download)\n",
    "        dataset_train.prepare()\n",
    "\n",
    "        # Validation dataset\n",
    "        dataset_val = CocoDataset()\n",
    "        dataset_val.load_coco(args.dataset, \"minival\", year=args.year, auto_download=args.download)\n",
    "        dataset_val.prepare()\n",
    "\n",
    "        # *** This training schedule is an example. Update to your needs ***\n",
    "\n",
    "        # Training - Stage 1\n",
    "        print(\"Training network heads\")\n",
    "        model.train_model(dataset_train, dataset_val,\n",
    "                    learning_rate=config.LEARNING_RATE,\n",
    "                    epochs=40,\n",
    "                    layers='heads')\n",
    "\n",
    "        # Training - Stage 2\n",
    "        # Finetune layers from ResNet stage 4 and up\n",
    "        print(\"Fine tune Resnet stage 4 and up\")\n",
    "        model.train_model(dataset_train, dataset_val,\n",
    "                    learning_rate=config.LEARNING_RATE,\n",
    "                    epochs=120,\n",
    "                    layers='4+')\n",
    "\n",
    "        # Training - Stage 3\n",
    "        # Fine tune all layers\n",
    "        print(\"Fine tune all layers\")\n",
    "        model.train_model(dataset_train, dataset_val,\n",
    "                    learning_rate=config.LEARNING_RATE / 10,\n",
    "                    epochs=160,\n",
    "                    layers='all')\n",
    "\n",
    "    elif args.command == \"evaluate\":\n",
    "        # Validation dataset\n",
    "        dataset_val = CocoDataset()\n",
    "        coco = dataset_val.load_coco(args.dataset, \"minival\", year=args.year, return_coco=True, auto_download=args.download)\n",
    "        dataset_val.prepare()\n",
    "        print(\"Running COCO evaluation on {} images.\".format(args.limit))\n",
    "        evaluate_coco(model, dataset_val, coco, \"bbox\", limit=int(args.limit))\n",
    "        evaluate_coco(model, dataset_val, coco, \"segm\", limit=int(args.limit))\n",
    "    else:\n",
    "        print(\"'{}' is not recognized. \"\n",
    "              \"Use 'train' or 'evaluate'\".format(args.command))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad5af0",
   "metadata": {},
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab911c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-44a0799c8715>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mask R-CNN\n",
    "Display and Visualization Functions.\n",
    "Copyright (c) 2017 Matterport, Inc.\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "Written by Waleed Abdulla\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import colorsys\n",
    "import numpy as np\n",
    "from skimage.measure import find_contours\n",
    "import matplotlib.pyplot as plt\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    plt.switch_backend('agg')\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Visualization\n",
    "############################################################\n",
    "\n",
    "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n",
    "                   interpolation=None):\n",
    "    \"\"\"Display the given set of images, optionally with titles.\n",
    "    images: list or array of image tensors in HWC format.\n",
    "    titles: optional. A list of titles to display with each image.\n",
    "    cols: number of images per row\n",
    "    cmap: Optional. Color map to use. For example, \"Blues\".\n",
    "    norm: Optional. A Normalize instance to map values to colors.\n",
    "    interpolation: Optional. Image interporlation to use for display.\n",
    "    \"\"\"\n",
    "    titles = titles if titles is not None else [\"\"] * len(images)\n",
    "    rows = len(images) // cols + 1\n",
    "    plt.figure(figsize=(14, 14 * rows // cols))\n",
    "    i = 1\n",
    "    for image, title in zip(images, titles):\n",
    "        plt.subplot(rows, cols, i)\n",
    "        plt.title(title, fontsize=9)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image.astype(np.uint8), cmap=cmap,\n",
    "                   norm=norm, interpolation=interpolation)\n",
    "        i += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def random_colors(N, bright=True):\n",
    "    \"\"\"\n",
    "    Generate random colors.\n",
    "    To get visually distinct colors, generate them in HSV space then\n",
    "    convert to RGB.\n",
    "    \"\"\"\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "\n",
    "def display_instances(image, boxes, masks, class_ids, class_names,\n",
    "                      scores=None, title=\"\",\n",
    "                      figsize=(16, 16), ax=None):\n",
    "    \"\"\"\n",
    "    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n",
    "    masks: [height, width, num_instances]\n",
    "    class_ids: [num_instances]\n",
    "    class_names: list of class names of the dataset\n",
    "    scores: (optional) confidence scores for each box\n",
    "    figsize: (optional) the size of the image.\n",
    "    \"\"\"\n",
    "    # Number of instances\n",
    "    N = boxes.shape[0]\n",
    "    if not N:\n",
    "        print(\"\\n*** No instances to display *** \\n\")\n",
    "    else:\n",
    "        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n",
    "\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots(1, figsize=figsize)\n",
    "\n",
    "    # Generate random colors\n",
    "    colors = random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    ax.set_ylim(height + 10, -10)\n",
    "    ax.set_xlim(-10, width + 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "\n",
    "        # Bounding box\n",
    "        if not np.any(boxes[i]):\n",
    "            # Skip this instance. Has no bbox. Likely lost in image cropping.\n",
    "            continue\n",
    "        y1, x1, y2, x2 = boxes[i]\n",
    "        p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                              alpha=0.7, linestyle=\"dashed\",\n",
    "                              edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(p)\n",
    "\n",
    "        # Label\n",
    "        class_id = class_ids[i]\n",
    "        score = scores[i] if scores is not None else None\n",
    "        label = class_names[class_id]\n",
    "        x = random.randint(x1, (x1 + x2) // 2)\n",
    "        caption = \"{} {:.3f}\".format(label, score) if score else label\n",
    "        ax.text(x1, y1 + 8, caption,\n",
    "                color='w', size=11, backgroundcolor=\"none\")\n",
    "\n",
    "        # Mask\n",
    "        mask = masks[:, :, i]\n",
    "        masked_image = apply_mask(masked_image, mask, color)\n",
    "\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        padded_mask = np.zeros(\n",
    "            (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "            ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def draw_rois(image, rois, refined_rois, mask, class_ids, class_names, limit=10):\n",
    "    \"\"\"\n",
    "    anchors: [n, (y1, x1, y2, x2)] list of anchors in image coordinates.\n",
    "    proposals: [n, 4] the same anchors but refined to fit objects better.\n",
    "    \"\"\"\n",
    "    masked_image = image.copy()\n",
    "\n",
    "    # Pick random anchors in case there are too many.\n",
    "    ids = np.arange(rois.shape[0], dtype=np.int32)\n",
    "    ids = np.random.choice(\n",
    "        ids, limit, replace=False) if ids.shape[0] > limit else ids\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "    if rois.shape[0] > limit:\n",
    "        plt.title(\"Showing {} random ROIs out of {}\".format(\n",
    "            len(ids), rois.shape[0]))\n",
    "    else:\n",
    "        plt.title(\"{} ROIs\".format(len(ids)))\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    ax.set_ylim(image.shape[0] + 20, -20)\n",
    "    ax.set_xlim(-50, image.shape[1] + 20)\n",
    "    ax.axis('off')\n",
    "\n",
    "    for i, id in enumerate(ids):\n",
    "        color = np.random.rand(3)\n",
    "        class_id = class_ids[id]\n",
    "        # ROI\n",
    "        y1, x1, y2, x2 = rois[id]\n",
    "        p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                              edgecolor=color if class_id else \"gray\",\n",
    "                              facecolor='none', linestyle=\"dashed\")\n",
    "        ax.add_patch(p)\n",
    "        # Refined ROI\n",
    "        if class_id:\n",
    "            ry1, rx1, ry2, rx2 = refined_rois[id]\n",
    "            p = patches.Rectangle((rx1, ry1), rx2 - rx1, ry2 - ry1, linewidth=2,\n",
    "                                  edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "            # Connect the top-left corners of the anchor and proposal for easy visualization\n",
    "            ax.add_line(lines.Line2D([x1, rx1], [y1, ry1], color=color))\n",
    "\n",
    "            # Label\n",
    "            label = class_names[class_id]\n",
    "            ax.text(rx1, ry1 + 8, \"{}\".format(label),\n",
    "                    color='w', size=11, backgroundcolor=\"none\")\n",
    "\n",
    "            # Mask\n",
    "            m = utils.unmold_mask(mask[id], rois[id]\n",
    "                                  [:4].astype(np.int32), image.shape)\n",
    "            masked_image = apply_mask(masked_image, m, color)\n",
    "\n",
    "    ax.imshow(masked_image)\n",
    "\n",
    "    # Print stats\n",
    "    print(\"Positive ROIs: \", class_ids[class_ids > 0].shape[0])\n",
    "    print(\"Negative ROIs: \", class_ids[class_ids == 0].shape[0])\n",
    "    print(\"Positive Ratio: {:.2f}\".format(\n",
    "        class_ids[class_ids > 0].shape[0] / class_ids.shape[0]))\n",
    "\n",
    "\n",
    "# TODO: Replace with matplotlib equivalent?\n",
    "def draw_box(image, box, color):\n",
    "    \"\"\"Draw 3-pixel width bounding boxes on the given image array.\n",
    "    color: list of 3 int values for RGB.\n",
    "    \"\"\"\n",
    "    y1, x1, y2, x2 = box\n",
    "    image[y1:y1 + 2, x1:x2] = color\n",
    "    image[y2:y2 + 2, x1:x2] = color\n",
    "    image[y1:y2, x1:x1 + 2] = color\n",
    "    image[y1:y2, x2:x2 + 2] = color\n",
    "    return image\n",
    "\n",
    "\n",
    "def display_top_masks(image, mask, class_ids, class_names, limit=4):\n",
    "    \"\"\"Display the given image and the top few class masks.\"\"\"\n",
    "    to_display = []\n",
    "    titles = []\n",
    "    to_display.append(image)\n",
    "    titles.append(\"H x W={}x{}\".format(image.shape[0], image.shape[1]))\n",
    "    # Pick top prominent classes in this image\n",
    "    unique_class_ids = np.unique(class_ids)\n",
    "    mask_area = [np.sum(mask[:, :, np.where(class_ids == i)[0]])\n",
    "                 for i in unique_class_ids]\n",
    "    top_ids = [v[0] for v in sorted(zip(unique_class_ids, mask_area),\n",
    "                                    key=lambda r: r[1], reverse=True) if v[1] > 0]\n",
    "    # Generate images and titles\n",
    "    for i in range(limit):\n",
    "        class_id = top_ids[i] if i < len(top_ids) else -1\n",
    "        # Pull masks of instances belonging to the same class.\n",
    "        m = mask[:, :, np.where(class_ids == class_id)[0]]\n",
    "        m = np.sum(m * np.arange(1, m.shape[-1] + 1), -1)\n",
    "        to_display.append(m)\n",
    "        titles.append(class_names[class_id] if class_id != -1 else \"-\")\n",
    "    display_images(to_display, titles=titles, cols=limit + 1, cmap=\"Blues_r\")\n",
    "\n",
    "\n",
    "def plot_precision_recall(AP, precisions, recalls):\n",
    "    \"\"\"Draw the precision-recall curve.\n",
    "    AP: Average precision at IoU >= 0.5\n",
    "    precisions: list of precision values\n",
    "    recalls: list of recall values\n",
    "    \"\"\"\n",
    "    # Plot the Precision-Recall curve\n",
    "    _, ax = plt.subplots(1)\n",
    "    ax.set_title(\"Precision-Recall Curve. AP@50 = {:.3f}\".format(AP))\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_xlim(0, 1.1)\n",
    "    _ = ax.plot(recalls, precisions)\n",
    "\n",
    "\n",
    "def plot_overlaps(gt_class_ids, pred_class_ids, pred_scores,\n",
    "                  overlaps, class_names, threshold=0.5):\n",
    "    \"\"\"Draw a grid showing how ground truth objects are classified.\n",
    "    gt_class_ids: [N] int. Ground truth class IDs\n",
    "    pred_class_id: [N] int. Predicted class IDs\n",
    "    pred_scores: [N] float. The probability scores of predicted classes\n",
    "    overlaps: [pred_boxes, gt_boxes] IoU overlaps of predictins and GT boxes.\n",
    "    class_names: list of all class names in the dataset\n",
    "    threshold: Float. The prediction probability required to predict a class\n",
    "    \"\"\"\n",
    "    gt_class_ids = gt_class_ids[gt_class_ids != 0]\n",
    "    pred_class_ids = pred_class_ids[pred_class_ids != 0]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(overlaps, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.yticks(np.arange(len(pred_class_ids)),\n",
    "               [\"{} ({:.2f})\".format(class_names[int(id)], pred_scores[i])\n",
    "                for i, id in enumerate(pred_class_ids)])\n",
    "    plt.xticks(np.arange(len(gt_class_ids)),\n",
    "               [class_names[int(id)] for id in gt_class_ids], rotation=90)\n",
    "\n",
    "    thresh = overlaps.max() / 2.\n",
    "    for i, j in itertools.product(range(overlaps.shape[0]),\n",
    "                                  range(overlaps.shape[1])):\n",
    "        text = \"\"\n",
    "        if overlaps[i, j] > threshold:\n",
    "            text = \"match\" if gt_class_ids[j] == pred_class_ids[i] else \"wrong\"\n",
    "        color = (\"white\" if overlaps[i, j] > thresh\n",
    "                 else \"black\" if overlaps[i, j] > 0\n",
    "                 else \"grey\")\n",
    "        plt.text(j, i, \"{:.3f}\\n{}\".format(overlaps[i, j], text),\n",
    "                 horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                 fontsize=9, color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Ground Truth\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "\n",
    "\n",
    "def draw_boxes(image, boxes=None, refined_boxes=None,\n",
    "               masks=None, captions=None, visibilities=None,\n",
    "               title=\"\", ax=None):\n",
    "    \"\"\"Draw bounding boxes and segmentation masks with differnt\n",
    "    customizations.\n",
    "    boxes: [N, (y1, x1, y2, x2, class_id)] in image coordinates.\n",
    "    refined_boxes: Like boxes, but draw with solid lines to show\n",
    "        that they're the result of refining 'boxes'.\n",
    "    masks: [N, height, width]\n",
    "    captions: List of N titles to display on each box\n",
    "    visibilities: (optional) List of values of 0, 1, or 2. Determine how\n",
    "        prominant each bounding box should be.\n",
    "    title: An optional title to show over the image\n",
    "    ax: (optional) Matplotlib axis to draw on.\n",
    "    \"\"\"\n",
    "    # Number of boxes\n",
    "    assert boxes is not None or refined_boxes is not None\n",
    "    N = boxes.shape[0] if boxes is not None else refined_boxes.shape[0]\n",
    "\n",
    "    # Matplotlib Axis\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots(1, figsize=(12, 12))\n",
    "\n",
    "    # Generate random colors\n",
    "    colors = random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    margin = image.shape[0] // 10\n",
    "    ax.set_ylim(image.shape[0] + margin, -margin)\n",
    "    ax.set_xlim(-margin, image.shape[1] + margin)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        # Box visibility\n",
    "        visibility = visibilities[i] if visibilities is not None else 1\n",
    "        if visibility == 0:\n",
    "            color = \"gray\"\n",
    "            style = \"dotted\"\n",
    "            alpha = 0.5\n",
    "        elif visibility == 1:\n",
    "            color = colors[i]\n",
    "            style = \"dotted\"\n",
    "            alpha = 1\n",
    "        elif visibility == 2:\n",
    "            color = colors[i]\n",
    "            style = \"solid\"\n",
    "            alpha = 1\n",
    "\n",
    "        # Boxes\n",
    "        if boxes is not None:\n",
    "            if not np.any(boxes[i]):\n",
    "                # Skip this instance. Has no bbox. Likely lost in cropping.\n",
    "                continue\n",
    "            y1, x1, y2, x2 = boxes[i]\n",
    "            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                                  alpha=alpha, linestyle=style,\n",
    "                                  edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "\n",
    "        # Refined boxes\n",
    "        if refined_boxes is not None and visibility > 0:\n",
    "            ry1, rx1, ry2, rx2 = refined_boxes[i].astype(np.int32)\n",
    "            p = patches.Rectangle((rx1, ry1), rx2 - rx1, ry2 - ry1, linewidth=2,\n",
    "                                  edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "            # Connect the top-left corners of the anchor and proposal\n",
    "            if boxes is not None:\n",
    "                ax.add_line(lines.Line2D([x1, rx1], [y1, ry1], color=color))\n",
    "\n",
    "        # Captions\n",
    "        if captions is not None:\n",
    "            caption = captions[i]\n",
    "            # If there are refined boxes, display captions on them\n",
    "            if refined_boxes is not None:\n",
    "                y1, x1, y2, x2 = ry1, rx1, ry2, rx2\n",
    "            x = random.randint(x1, (x1 + x2) // 2)\n",
    "            ax.text(x1, y1, caption, size=11, verticalalignment='top',\n",
    "                    color='w', backgroundcolor=\"none\",\n",
    "                    bbox={'facecolor': color, 'alpha': 0.5,\n",
    "                          'pad': 2, 'edgecolor': 'none'})\n",
    "\n",
    "        # Masks\n",
    "        if masks is not None:\n",
    "            mask = masks[:, :, i]\n",
    "            masked_image = apply_mask(masked_image, mask, color)\n",
    "            # Mask Polygon\n",
    "            # Pad to ensure proper polygons for masks that touch image edges.\n",
    "            padded_mask = np.zeros(\n",
    "                (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "            padded_mask[1:-1, 1:-1] = mask\n",
    "            contours = find_contours(padded_mask, 0.5)\n",
    "            for verts in contours:\n",
    "                # Subtract the padding and flip (y, x) to (x, y)\n",
    "                verts = np.fliplr(verts) - 1\n",
    "                p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "                ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8))\n",
    "\n",
    "def plot_loss(loss, val_loss, save=True, log_dir=None):\n",
    "    loss = np.array(loss)\n",
    "    val_loss = np.array(val_loss)\n",
    "\n",
    "    plt.figure(\"loss\")\n",
    "    plt.gcf().clear()\n",
    "    plt.plot(loss[:, 0], label='train')\n",
    "    plt.plot(val_loss[:, 0], label='valid')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        save_path = os.path.join(log_dir, \"loss.png\")\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    plt.figure(\"rpn_class_loss\")\n",
    "    plt.gcf().clear()\n",
    "    plt.plot(loss[:, 1], label='train')\n",
    "    plt.plot(val_loss[:, 1], label='valid')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        save_path = os.path.join(log_dir, \"rpn_class_loss.png\")\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    plt.figure(\"rpn_bbox_loss\")\n",
    "    plt.gcf().clear()\n",
    "    plt.plot(loss[:, 2], label='train')\n",
    "    plt.plot(val_loss[:, 2], label='valid')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        save_path = os.path.join(log_dir, \"rpn_bbox_loss.png\")\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    plt.figure(\"mrcnn_class_loss\")\n",
    "    plt.gcf().clear()\n",
    "    plt.plot(loss[:, 3], label='train')\n",
    "    plt.plot(val_loss[:, 3], label='valid')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        save_path = os.path.join(log_dir, \"mrcnn_class_loss.png\")\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    plt.figure(\"mrcnn_bbox_loss\")\n",
    "    plt.gcf().clear()\n",
    "    plt.plot(loss[:, 4], label='train')\n",
    "    plt.plot(val_loss[:, 4], label='valid')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        save_path = os.path.join(log_dir, \"mrcnn_bbox_loss.png\")\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    plt.figure(\"mrcnn_mask_loss\")\n",
    "    plt.gcf().clear()\n",
    "    plt.plot(loss[:, 5], label='train')\n",
    "    plt.plot(val_loss[:, 5], label='valid')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        save_path = os.path.join(log_dir, \"mrcnn_mask_loss.png\")\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358cc7a8",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6550a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[256 256]\n",
      " [128 128]\n",
      " [ 64  64]\n",
      " [ 32  32]\n",
      " [ 16  16]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGENET_MODEL_PATH            C:\\Users\\bhfux\\Desktop\\\\\\XAI\\resnet50_imagenet.pth\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MaskRCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0ab8dea0bd90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# Create model object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaskRCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGPU_COUNT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MaskRCNN' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Path to trained weights file\n",
    "# Download this file and place in the root of your\n",
    "# project (See README file for details)\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.pth\")\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "class InferenceConfig(CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    # GPU_COUNT = 0 for CPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()\n",
    "\n",
    "# Create model object.\n",
    "model = MaskRCNN(model_dir=MODEL_DIR, config=config)\n",
    "if config.GPU_COUNT:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_state_dict(torch.load(COCO_MODEL_PATH))\n",
    "\n",
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "# Load a random image from the images folder\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
    "\n",
    "# Run detection\n",
    "results = model.detect([image])\n",
    "\n",
    "# Visualize results\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n",
    "                            class_names, r['scores'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d9c61",
   "metadata": {},
   "source": [
    "# convert_from_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03d72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mask R-CNN\n",
    "Base Configurations class.\n",
    "Copyright (c) 2017 Matterport, Inc.\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "Written by Waleed Abdulla\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Base Configuration Class\n",
    "# Don't use this class directly. Instead, sub-class it and override\n",
    "# the configurations you need to change.\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Base configuration class. For custom configurations, create a\n",
    "    sub-class that inherits from this one and override properties\n",
    "    that need to be changed.\n",
    "    \"\"\"\n",
    "    # Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.\n",
    "    # Useful if your code needs to do things differently depending on which\n",
    "    # experiment is running.\n",
    "    NAME = None  # Override in sub-classes\n",
    "\n",
    "    # Path to pretrained imagenet model\n",
    "    IMAGENET_MODEL_PATH = os.path.join(os.getcwd(), \"resnet50_imagenet.pth\")\n",
    "\n",
    "    # NUMBER OF GPUs to use. For CPU use 0\n",
    "    GPU_COUNT = 1\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "    # Number of validation steps to run at the end of every training epoch.\n",
    "    # A bigger number improves accuracy of validation stats, but slows\n",
    "    # down the training.\n",
    "    VALIDATION_STEPS = 50\n",
    "\n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a Resnet101 backbone.\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1  # Override in sub-classes\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "    # Anchor stride\n",
    "    # If 1 then anchors are created for each cell in the backbone feature map.\n",
    "    # If 2, then anchors are created for every other cell, and so on.\n",
    "    RPN_ANCHOR_STRIDE = 1\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can reduce this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 256\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 1000\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Input image resing\n",
    "    # Images are resized such that the smallest side is >= IMAGE_MIN_DIM and\n",
    "    # the longest side is <= IMAGE_MAX_DIM. In case both conditions can't\n",
    "    # be satisfied together the IMAGE_MAX_DIM is enforced.\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    # If True, pad images with zeros such that they're (max_dim by max_dim)\n",
    "    IMAGE_PADDING = True  # currently, the False option is not supported\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([123.7, 116.8, 103.9])\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Percent of positive ROIs used to train classifier/mask heads\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "\n",
    "    # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "    MASK_SHAPE = [28, 28]\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 100\n",
    "\n",
    "    # Bounding box refinement standard deviation for RPN and final detections.\n",
    "    RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "\n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "\n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n",
    "    # weights to explode. Likely due to differences in optimzer\n",
    "    # implementation.\n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "\n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "    # Use RPN ROIs or externally generated ROIs for training\n",
    "    # Keep this True for most situations. Set to False if you want to train\n",
    "    # the head branches on ROI generated by code rather than the ROIs from\n",
    "    # the RPN. For example, to debug the classifier head without having to\n",
    "    # train the RPN.\n",
    "    USE_RPN_ROIS = True\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Set values of computed attributes.\"\"\"\n",
    "        # Effective batch size\n",
    "        if self.GPU_COUNT > 0:\n",
    "            self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT\n",
    "        else:\n",
    "            self.BATCH_SIZE = self.IMAGES_PER_GPU\n",
    "\n",
    "        # Adjust step size based on batch size\n",
    "        self.STEPS_PER_EPOCH = self.BATCH_SIZE * self.STEPS_PER_EPOCH\n",
    "\n",
    "        # Input image size\n",
    "        self.IMAGE_SHAPE = np.array(\n",
    "            [self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM, 3])\n",
    "\n",
    "        # Compute backbone size from input image size\n",
    "        self.BACKBONE_SHAPES = np.array(\n",
    "            [[int(math.ceil(self.IMAGE_SHAPE[0] / stride)),\n",
    "              int(math.ceil(self.IMAGE_SHAPE[1] / stride))]\n",
    "             for stride in self.BACKBONE_STRIDES])\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2318c18",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import coco\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Path to trained weights file\n",
    "# Download this file and place in the root of your\n",
    "# project (See README file for details)\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.pth\")\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    # GPU_COUNT = 0 for CPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()\n",
    "\n",
    "# Create model object.\n",
    "model = modellib.MaskRCNN(model_dir=MODEL_DIR, config=config)\n",
    "if config.GPU_COUNT:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_state_dict(torch.load(COCO_MODEL_PATH))\n",
    "\n",
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "# Load a random image from the images folder\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
    "\n",
    "# Run detection\n",
    "results = model.detect([image])\n",
    "\n",
    "# Visualize results\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n",
    "                            class_names, r['scores'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
